# Overview
The National Science Foundation's [National Ecological Observatory Network (NEON)](https://www.neonscience.org/about) collects standardized, open-access ecological data at 81 freshwater and terrestrial field sites across the country. In addition to an amazing array of on-the-ground surveys, they also periodically collect Lidar data at the sites. All data is publicly available through the NEON Data Portal.

For this exercise, we will imagine that we are interested in studying canopy structure (tree height) at the San Joaquin Experimental Range in California. We're interested in figuring out if we can rely on the Lidar data NEON is collecting by comparing tree height estimates to on-the-ground field surveys. If the estimates between the two methods are similar, we could save ourselves a lot of time and effort measuring trees!

credit: this lab is based on [materials](https://www.neonscience.org/resources/learning-hub/tutorials/introduction-light-detection-and-ranging-lidar-explore-point) developed by Edmund Hart, Leah Wasser, and Donal O'Leary for NEON.

# Task

To estimate tree height from Lidar data, we will create a canopy height model (CHM) from Lidar-derived digital surface and terrain models. We will then extract tree height estimates within the locations of on-the-ground surveys and compare Lidar estimates to measured tree height in each plot. 

### Data

**Lidar data**

- Digital surface model (DSM) `SJER2013_DSM.tif`
- Digital terrain model (DTM) `SJER2013_DTM.tif`
- DSMs represent the elevation of the top of all objects
- DTMs represent the elevation of the ground (or terrain)

**Vegetation plot geometries**

- `SJERPlotCentroids_Buffer.shp`
- contains locations of vegetation surveys
- polygons representing 20m buffer around plot centroids


**Vegetation surveys**

- `D17_2013_vegStr.csv`
- measurements for individual trees in each plot
- metadata available in `D17_2013_vegStr_metadata_desc.csv`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Start by loading all necessary packages and setting working directory.
```{r load-packages, warning=FALSE, message=FALSE}
rm(list = ls())
library(terra)
library(sf)
library(tidyverse)
library(tmap)
library(here)

here::i_am("/Users/sofiaingersoll/1/EDS223/eds223-labs/week10-template.Rmd")
```
#### Load Landsat data
Let's create a raster stack based on the 6 bands we will be working with. Each file name ends with the band number (e.g. `B1.tif`). Notice that we are missing a file for band 6. Band 6 corresponds to thermal data, which we will not be working with for this lab. To create a raster stack, we will create a list of the files that we would like to work with and read them all in at once using the `rast` function. We'll then update the names of the layers to match the spectral bands and plot a true color image to see what we're working with.

```{r include=TRUE}
# list files for each band, including the full file path
filelist <- list.files("/Users/sofiaingersoll/1/EDS223/week10/landsat-data/", full.names = TRUE)

# read in and store as a raster stack
landsat_20070925 <- rast(filelist)

# update layer names to match band
names(landsat_20070925) <- c("blue", "green", "red", "NIR", "SWIR1", "SWIR2")

# plot true color image
plotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = "lin")
```

#### Load study area
We want to contstrain our analysis to the southern portion of the county where we have training data, so we'll read in a file that defines the area we would like to study.

```{r include=TRUE}

# read in shapefile for southern portion of SB county


SB_county_south <- st_read("/Users/sofiaingersoll/1/EDS223/week10/SB_county_south.shp")

# project to match the Landsat data
SB_county_south <- st_transform(SB_county_south, crs = crs(landsat_20070925))
```

#### Crop and mask Landsat data to study area
Now, we can crop and mask the Landsat data to our study area. This reduces the amount of data we'll be working with and therefore saves computational time. We can also remove any objects we're no longer working with to save space. 
```{r include=TRUE}

# crop Landsat scene to the extent of the SB county shapefile
landsat_cropped <- crop(landsat_20070925, SB_county_south)

# mask the raster to southern portion of SB county
landsat_masked <- mask(landsat_cropped, SB_county_south)

# remove unnecessary object from environment
rm(landsat_20070925, SB_county_south, landsat_cropped)
```

#### Convert Landsat values to reflectance
Now we need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any [scaling factors](https://www.usgs.gov/faqs/how-do-i-use-scale-factor-landsat-level-2-science-products#:~:text=Landsat%20Collection%202%20surface%20temperature,the%20scale%20factor%20is%20applied.) to convert to reflectance.\

In this case, we are working with [Landsat Collection 2](https://www.usgs.gov/landsat-missions/landsat-collection-2). The valid range of pixel values for this collection 7,273-43,636, with a multiplicative scale factor of 0.0000275 and an additive scale factor of -0.2. So we reclassify any erroneous values as `NA` and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%.

```{r include=TRUE}
# reclassify erroneous values as NA
rcl <- matrix(c(-Inf, 7273, NA,
                 43636, Inf, NA), ncol = 3, byrow = TRUE)

landsat <- classify(landsat_masked, rcl = rcl)

# adjust values based on scaling factor
landsat <- (landsat * 0.0000275 - 0.2) * 100

# plot true color image to check results
plotRGB(landsat, r = 3, g = 2, b = 1, stretch = "lin")

# check values are 0 - 100
summary(landsat)
```


## Classify image

#### Extract reflectance values for training data
We will load the shapefile identifying different locations within our study area as containing one of our 4 land cover types. We can then extract the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.

```{r include=TRUE}
# read in and transform training data
training_data <- st_read("./data/week9/trainingdata.shp") %>%
  st_transform(., crs = crs(landsat))

# extract reflectance values at training sites
training_data_values <- extract(landsat, training_data, df = TRUE)

# convert training data to data frame
training_data_attributes <- training_data %>%
  st_drop_geometry()

# join training data attributes and extracted reflectance values
SB_training_data <- left_join(training_data_values, training_data_attributes,
                              by = c("ID" = "id")) %>%
  mutate(type = as.factor(type)) # convert landcover type to factor
```

#### Train decision tree classifier
To train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are). The `rpart` function implements the [CART algorithm](https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff). The `rpart` function needs to know the model formula and training data you would like to use. Because we are performing a classification, we set `method = "class"`. We also set `na.action = na.omit` to remove any pixels with `NA`s from the analysis.\

To understand how our decision tree will classify pixels, we can plot the results. The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.   

```{r include=TRUE}
# establish model formula
SB_formula <- type ~ red + green + blue + NIR + SWIR1 + SWIR2

# train decision tree
SB_decision_tree <- rpart(formula = SB_formula,
                          data = SB_training_data,
                          method = "class",
                          na.action = na.omit)

# plot decision tree
prp(SB_decision_tree)
```

#### Apply decision tree
Now that we have created our decision tree, we can apply it to our entire image. The `terra` package includes a `predict()` function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The `predict()` function will return a raster layer with integer values. These integer values correspond to the *factor levels* in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data. 

```{r include=TRUE}
# classify image based on decision tree
SB_classification <- predict(landsat, SB_decision_tree, type = "class", na.rm = TRUE)

# inspect level to understand the order of classes in prediction
levels(SB_training_data$type)

```

#### Plot results
Now we can plot the results and check out our land cover map!
```{r}
# plot results

tm_shape(SB_classification) +
  tm_raster(col.scale = tm_scale_categorical(values = c("#8DB580", "#F2DDA4", "#7E8987", "#6A8EAE")),
            col.legend = tm_legend(labels = c("green vegetation", "soil/dead grass", "urban", "water"),
                                   title = "Landcover type")) +
  tm_layout(legend.position = c("left", "bottom"))

```

Load Lidar data. 
```{r load-lidar}
# digital surface model (DSM)
dsm <- rast(here("/Users/sofiaingersoll/1/EDS223/week10/SJER2013_DSM.tif"))

# digital terrain model (DTM)
dtm <- rast(here("/Users/sofiaingersoll/1/EDS223/week10/SJER2013_DTM.tif"))

```

Check if the DSM and DTM have the same resolution, position, and extent by creating a raster stack.
```{r check-raster}

c(dsm, dtm)
```


Create the canopy height model (CHM) or the height of all objects by finding the difference between the DSM and DTM.
```{r compute-chm}

chm <- dsm - dtm

plot(chm)
```

Load the vegetation survey data, including the locations of study plots and the surveys of individual trees in each plot.
```{r load-centroids}
# read in plot centroids
plot_centroids <- st_read(here("/Users/sofiaingersoll/1/EDS223/week10/PlotCentroids/SJERPlotCentroids_Buffer.shp"))


# test if the plot CRS matches the Lidar CRS
st_crs(plot_centroids) == st_crs(chm)

tm_shape(chm) +
  tm_raster() +
  tm_shape(plot_centroids) +
  tm_polygons()


```

Load in the vegetation survey data and find the maximum tree height in each plot.
```{r load-surveys}
# read in the vegetation surveys, which include the height of each tree
options(stringsAsFactors = FALSE)  # this controls our data when being read in
veg_surveys <- read.csv((here("/Users/sofiaingersoll/1/EDS223/week10/VegetationData/D17_2013_vegStr.csv"))) %>% 
  group_by(plotid) %>% 
  summarize('survey_height' = max(stemheight, na.rm = TRUE))     # one output of max height for plot ids

# now we only have 18 observations

# setting this option will keep all character strings as characters
extract_chm_height <- terra::extract(chm, plot_centroids, fun = max) %>% 
  rename(chm_height = SJER2013_DSM) %>% 
  dplyr::select(chm_height)


# read in survey data and find the maximum tree height in each plot
height_estimates <- cbind(plot_centroids, extract_chm_height) %>% 
  left_join(., veg_surveys, by = c('Plot_ID' = 'plotid'))
```

Now find the maximum tree height in each plot as determined by the CHM.
```{r extract-height}

ggplot(height_estimates, aes(x = survey_height, y = chm_height)) +
geom_point(color = 'pink') +
  geom_smooth(method = lm) +
  geom_abline(slope = 1, intercept = 0, color = 'magenta') +
  labs(x = 'Maximum Tree Height - Survey (m)',
       y = 'Maximum Tree Height - LiDAR (m)')

```

create a function to take 2 data sets 1 poly 1 raster and make boxplot  based on specific layer
```{r}
# prints out a boxplot summary for each category
summary_boxplot = function(polygon, raster, m_layer, m_label) {
  #raserize polygon by layer -- this gives us resolution and data on layers
  id_rast = rasterize(polygon, raster, field = 'suid_nma')

   # do mean zonal stats, double << assignment operater allows output to be "global" vs others that are only accessible using tis function
  zonal_layer <<- zonal(raster, id_rast, fun = "mean", na.rm = TRUE)                   
  
  # join withpolygon database
  poly_join <<- full_join(polygon, zonal_layer) %>% 
    select(suid_na, gHM, paste(my_layer))            # paste this as a recognizable layer to access
  
  # create boxplot based on layer
  p1 <- ggplot(poly_join) +
      geom_boxplot(aes(gHM, .data[[my_layer]])) + # this is the only way we can affectivly call layer
    theme_bw() +
    labs(x = "Human Modification Index",
         y = my_label)
  return(p1)
  
}      
  # where is cpad_super and ghm              }
#test_sum <- summary_boxplot(cpad_super, ghm, "access_typ", 'Access Type')


```

select some layers and use function
```{r}
names(cpad_super)

access <- summary_boxplot(cpad_super, gmh, 'access_typ', 'Access Type')
access

layer = summary_boxplot(cpad_super, ghm, "layer", "Management Agency Type")
layer
```

Plotting
```{r}

p1 <- ggplot(data = cpad_super) +
  geom_sf(aes(color = access_typ,
              fill = access_typ)) +
  theme_bw() +
  labs( color = "Access Type",     # need to put both or you get two legends
        fill = "Access Type") +
  annotation_scale(plot_unit = "km") + # this scale factor is very inaccurate
  annotaiton_north_arrow(
    location = 'tr',
    style = ggspatial::north_arrow_nautical(
      fill = c('grey40', 'white'),
      line_col = ('grey20')
    )
  ) +
  coord_sf() +# this sets coordinate system so it's following spatial and not catesian. best to add incase geom_sf doesn't properly call it on the backend 
  scale_color_viridis_d() +
  scale_fill_viridis_d()

# view
# facet_wrap technically don't need bc colors differentiate, but this shows the different locations effect without all the overlap
p1 + 
  facet_wrap(~access_typ) + 
  theme(strip.background = element_rect(fill = 'transparent'))
```
Plot as stars 
```{r}
ggplot() +
  geom_stars(data = st_as_stars(ghm)) +    # geom_stars/raster/file/sf are all options for plotting
  coord_equal() +               # controls aspect ratio of the layers to be plotted
  theme_bw() +
  scale_fill_viridis_c() +
  labs(x = "",
       y = "",
       fill = 'Global Human Modification')

# we can add all of the same specialization from the plot before to this 
```



Combine tree height estimates from the Lidar and plot surveys.
```{r join-data}




```

Plot results! Let's compare the estimates between the two methods (Lidar and on-the-ground surveys). To make the comparison, we'll add a 1:1 line. If all the points fall along this line it means that both methods give the same answer. We'll also add a regression line with confidence intervals to compare how the overall fit between methods compares to the 1:1 line.

```{r plot-results}


```

We've now compared Lidar estimates of tree height to on-the-ground measurements! It looks like the Lidar estimates tend to underestimate tree height for shorter trees and overestimates tree height for taller trees. Or maybe human observers underestimate the height of tall trees because they're challenging to measure? Or maybe the digital terrain model misjudged the elevation of the ground? There could be many reasons that the answers don't line up! It's then up to the researcher to figure out if the mismatch is important for their problem.
